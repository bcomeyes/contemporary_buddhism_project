{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # PDF to Text Converter with OpenAI Embeddings\n",
    "\n",
    "\n",
    "\n",
    " This notebook extracts text from PDF files (including large ones) and can generate embeddings using OpenAI's API.\n",
    "\n",
    " The extracted text is saved to a text file, and optionally embeddings can be generated\n",
    "\n",
    " for the content to enable semantic search or analysis.\n",
    "\n",
    "\n",
    "\n",
    " ## Requirements\n",
    "\n",
    " You'll need to install these packages first:\n",
    "\n",
    " ```\n",
    "\n",
    " pip install PyPDF2 pymupdf pdf2image pytesseract pillow openai tqdm tiktoken numpy python-dotenv\n",
    "\n",
    " ```\n",
    "\n",
    "\n",
    "\n",
    " Note: For OCR functionality (handling scanned PDFs), you'll also need to install Tesseract OCR:\n",
    "\n",
    " - Windows: https://github.com/UB-Mannheim/tesseract/wiki\n",
    "\n",
    " - Mac: `brew install tesseract`\n",
    "\n",
    " - Linux: `sudo apt install tesseract-ocr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "import openai\n",
    "from tqdm.notebook import tqdm  # Using notebook version for better display in Jupyter\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Load environment variables from .env file in the current directory\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment variable\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# Set Tesseract executable path\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"  # OpenAI embedding model to use\n",
    "MAX_TOKENS = 8191  # Maximum tokens per chunk for embedding\n",
    "CHUNK_OVERLAP = 200  # Number of tokens to overlap between chunks\n",
    "DPI = 300  # DPI for OCR conversion (higher is better quality but slower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Helper Functions for PDF Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path, use_ocr=False, ocr_threshold=10):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        use_ocr: Whether to use OCR for text extraction (for scanned PDFs)\n",
    "        ocr_threshold: Character count threshold below which to try OCR (per page)\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text as a string\n",
    "    \"\"\"\n",
    "    total_text = []\n",
    "    \n",
    "    # Try extracting text with PyMuPDF (faster and better than PyPDF2 for most PDFs)\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        total_pages = len(doc)\n",
    "        \n",
    "        print(f\"Extracting text from {total_pages} pages...\")\n",
    "        for page_num in tqdm(range(total_pages)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Check if page has little text and might need OCR\n",
    "            if use_ocr and len(text.strip()) < ocr_threshold:\n",
    "                print(f\"Page {page_num+1} might be scanned. Attempting OCR...\")\n",
    "                # Convert page to image\n",
    "                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))\n",
    "                img = Image.open(io.BytesIO(pix.tobytes()))\n",
    "                # Apply OCR\n",
    "                text = pytesseract.image_to_string(img)\n",
    "            \n",
    "            total_text.append(text)\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with PyMuPDF: {e}\")\n",
    "        print(\"Falling back to PyPDF2...\")\n",
    "        \n",
    "        # Fallback to PyPDF2\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                total_pages = len(reader.pages)\n",
    "                \n",
    "                for page_num in tqdm(range(total_pages)):\n",
    "                    page = reader.pages[page_num]\n",
    "                    text = page.extract_text() or \"\"\n",
    "                    \n",
    "                    # If text extraction failed or returned minimal text, try OCR\n",
    "                    if use_ocr and len(text.strip()) < ocr_threshold:\n",
    "                        print(f\"Page {page_num+1} might be scanned. Attempting OCR...\")\n",
    "                        # Convert PDF page to image using pdf2image\n",
    "                        images = convert_from_path(pdf_path, dpi=DPI, first_page=page_num+1, last_page=page_num+1)\n",
    "                        # Apply OCR to the image\n",
    "                        text = pytesseract.image_to_string(images[0])\n",
    "                    \n",
    "                    total_text.append(text)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error with PyPDF2: {e}\")\n",
    "            # If everything fails, try full OCR if enabled\n",
    "            if use_ocr:\n",
    "                print(\"Attempting full document OCR...\")\n",
    "                try:\n",
    "                    images = convert_from_path(pdf_path, dpi=DPI)\n",
    "                    for i, img in enumerate(tqdm(images)):\n",
    "                        text = pytesseract.image_to_string(img)\n",
    "                        total_text.append(text)\n",
    "                except Exception as e:\n",
    "                    print(f\"OCR failed: {e}\")\n",
    "    \n",
    "    # Join all text with double newlines between pages\n",
    "    return \"\\n\\n\".join(total_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean the extracted text\"\"\"\n",
    "    # Replace multiple newlines with a single one\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    # Replace multiple spaces with a single one\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    # Fix any broken words that might have been split across lines\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Helper Functions for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_count(text, encoding_name=\"cl100k_base\"):\n",
    "    \"\"\"Count the number of tokens in a text string\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text, max_tokens=MAX_TOKENS, overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"Split text into chunks respecting token limits with overlap\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    \n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Get chunk of tokens (respecting max_tokens)\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        # Decode chunk back to text\n",
    "        chunk = encoding.decode(chunk_tokens)\n",
    "        chunks.append(chunk)\n",
    "        # Move forward by max_tokens - overlap\n",
    "        i += max_tokens - overlap\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(chunks, api_key=None):\n",
    "    \"\"\"Create embeddings for text chunks using OpenAI API\"\"\"\n",
    "    # Use provided API key or environment variable\n",
    "    if api_key:\n",
    "        openai.api_key = api_key\n",
    "    else:\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "        \n",
    "    if not openai.api_key:\n",
    "        print(\"Error: No OpenAI API key provided. Set OPENAI_API_KEY in your .env file.\")\n",
    "        return []\n",
    "        \n",
    "    embeddings = []\n",
    "    \n",
    "    print(f\"Creating embeddings for {len(chunks)} chunks...\")\n",
    "    for i, chunk in enumerate(tqdm(chunks)):\n",
    "        try:\n",
    "            # Add a small delay to respect API rate limits\n",
    "            if i > 0 and i % 10 == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "            response = openai.embeddings.create(\n",
    "                model=EMBEDDING_MODEL,\n",
    "                input=chunk\n",
    "            )\n",
    "            embedding = response.data[0].embedding\n",
    "            embeddings.append({\n",
    "                \"chunk\": chunk,\n",
    "                \"embedding\": embedding,\n",
    "                \"chunk_index\": i\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embedding for chunk {i}: {e}\")\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Semantic Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_embeddings(query, embeddings, api_key=None, top_n=5):\n",
    "    \"\"\"Search embeddings for relevant text chunks based on a query\"\"\"\n",
    "    # Get embedding for the query\n",
    "    if api_key:\n",
    "        openai.api_key = api_key\n",
    "    else:\n",
    "        # Use environment variable if no API key is provided\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "    \n",
    "    if not openai.api_key:\n",
    "        print(\"Error: No OpenAI API key provided. Set OPENAI_API_KEY in your .env file.\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        response = openai.embeddings.create(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            input=query\n",
    "        )\n",
    "        query_embedding = response.data[0].embedding\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_embedding_array = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        similarities = []\n",
    "        for i, item in enumerate(embeddings):\n",
    "            embed_array = np.array(item[\"embedding\"])\n",
    "            # Cosine similarity\n",
    "            similarity = np.dot(query_embedding_array, embed_array) / (\n",
    "                np.linalg.norm(query_embedding_array) * np.linalg.norm(embed_array)\n",
    "            )\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (highest first)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top N results\n",
    "        results = []\n",
    "        for idx, score in similarities[:top_n]:\n",
    "            results.append({\n",
    "                \"chunk\": embeddings[idx][\"chunk\"],\n",
    "                \"similarity\": float(score),\n",
    "                \"chunk_index\": embeddings[idx][\"chunk_index\"]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Interactive Notebook Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive part - you can edit these parameters\n",
    "pdf_pattern = \"*.pdf\"  # Glob pattern to match PDF files (e.g., \"*.pdf\", \"documents/*.pdf\")\n",
    "output_dir = \".\"  # Directory to save output files\n",
    "generate_embeddings = True  # Set to True if you want to generate embeddings\n",
    "use_ocr = True  # Set to True for scanned PDFs that need OCR\n",
    "batch_process = True  # Set to True to process multiple PDFs matching the pattern\n",
    "single_file = \"your_document.pdf\"  # Path to a single PDF file if not batch processing\n",
    "\n",
    "# Use API key from environment variable by default\n",
    "# You can override it here if needed\n",
    "openai_api_key = OPENAI_API_KEY\n",
    "\n",
    "# Check if API key is available when embeddings are requested\n",
    "if generate_embeddings and not openai_api_key:\n",
    "    print(\"Warning: No OpenAI API key found in environment variables.\")\n",
    "    print(\"Please add OPENAI_API_KEY to your .env file or set it here manually.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Process PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = Path(output_dir)\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Function to process a single PDF file\n",
    "def process_pdf(pdf_file):\n",
    "    print(f\"\\nProcessing: {pdf_file}\")\n",
    "    \n",
    "    # Get filename without extension\n",
    "    filename = Path(pdf_file).stem\n",
    "    \n",
    "    # Extract text from PDF\n",
    "    print(f\"Extracting text from {pdf_file}...\")\n",
    "    text = extract_text_from_pdf(pdf_file, use_ocr=use_ocr)\n",
    "    \n",
    "    # Clean the text\n",
    "    if text:\n",
    "        print(\"Cleaning extracted text...\")\n",
    "        text = clean_text(text)\n",
    "        \n",
    "        # Save text content\n",
    "        text_path = output_dir / f\"{filename}.txt\"\n",
    "        with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"Text saved to {text_path}\")\n",
    "        \n",
    "        # Display a sample of the text\n",
    "        print(\"\\nSample of extracted text:\")\n",
    "        print(text[:500] + \"...\" if len(text) > 500 else text)\n",
    "        print(\"\\nTotal characters:\", len(text))\n",
    "        token_count = get_token_count(text)\n",
    "        print(f\"Total tokens: {token_count}\")\n",
    "        \n",
    "        # Create embeddings if requested\n",
    "        if generate_embeddings:\n",
    "            if not openai_api_key:\n",
    "                print(\"Error: OpenAI API key is required for creating embeddings\")\n",
    "                print(\"Please set OPENAI_API_KEY in your .env file or provide it in the parameters cell.\")\n",
    "            else:\n",
    "                # Split text into chunks\n",
    "                chunks = split_into_chunks(text)\n",
    "                print(f\"Split into {len(chunks)} chunks\")\n",
    "                \n",
    "                # Generate embeddings\n",
    "                embeddings = create_embeddings(chunks, openai_api_key)\n",
    "                \n",
    "                # Save embeddings\n",
    "                embeddings_path = output_dir / f\"{filename}_embeddings.json\"\n",
    "                with open(embeddings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(embeddings, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"Embeddings saved to {embeddings_path}\")\n",
    "                \n",
    "                # Save a version with just the text chunks for reference\n",
    "                chunks_path = output_dir / f\"{filename}_chunks.json\"\n",
    "                chunks_data = [{\"chunk_index\": i, \"chunk\": chunk} for i, chunk in enumerate(chunks)]\n",
    "                with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"Text chunks saved to {chunks_path}\")\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to extract text from {pdf_file}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 PDF files matching pattern: *.pdf\n",
      "\n",
      "Processing: Apoha-Buddhist Nominalism and Human Cognition.pdf\n",
      "Extracting text from Apoha-Buddhist Nominalism and Human Cognition.pdf...\n",
      "Extracting text from 341 pages...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edee89f04e5473e996f9ff0073fd50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 might be scanned. Attempting OCR...\n",
      "Page 2 might be scanned. Attempting OCR...\n",
      "Page 3 might be scanned. Attempting OCR...\n",
      "Page 4 might be scanned. Attempting OCR...\n",
      "Page 5 might be scanned. Attempting OCR...\n",
      "Page 6 might be scanned. Attempting OCR...\n",
      "Page 7 might be scanned. Attempting OCR...\n",
      "Page 8 might be scanned. Attempting OCR...\n",
      "Page 9 might be scanned. Attempting OCR...\n",
      "Page 10 might be scanned. Attempting OCR...\n",
      "Page 11 might be scanned. Attempting OCR...\n",
      "Page 12 might be scanned. Attempting OCR...\n",
      "Page 13 might be scanned. Attempting OCR...\n",
      "Page 14 might be scanned. Attempting OCR...\n",
      "Page 15 might be scanned. Attempting OCR...\n",
      "Page 16 might be scanned. Attempting OCR...\n",
      "Page 17 might be scanned. Attempting OCR...\n",
      "Page 18 might be scanned. Attempting OCR...\n",
      "Page 19 might be scanned. Attempting OCR...\n",
      "Page 20 might be scanned. Attempting OCR...\n",
      "Page 21 might be scanned. Attempting OCR...\n",
      "Page 22 might be scanned. Attempting OCR...\n",
      "Page 23 might be scanned. Attempting OCR...\n",
      "Page 24 might be scanned. Attempting OCR...\n",
      "Page 25 might be scanned. Attempting OCR...\n",
      "Page 26 might be scanned. Attempting OCR...\n",
      "Page 27 might be scanned. Attempting OCR...\n",
      "Page 28 might be scanned. Attempting OCR...\n",
      "Page 29 might be scanned. Attempting OCR...\n",
      "Page 30 might be scanned. Attempting OCR...\n",
      "Page 31 might be scanned. Attempting OCR...\n",
      "Page 32 might be scanned. Attempting OCR...\n",
      "Page 33 might be scanned. Attempting OCR...\n",
      "Page 34 might be scanned. Attempting OCR...\n",
      "Page 35 might be scanned. Attempting OCR...\n",
      "Page 36 might be scanned. Attempting OCR...\n",
      "Page 37 might be scanned. Attempting OCR...\n",
      "Page 38 might be scanned. Attempting OCR...\n",
      "Page 39 might be scanned. Attempting OCR...\n",
      "Page 40 might be scanned. Attempting OCR...\n",
      "Page 41 might be scanned. Attempting OCR...\n",
      "Page 42 might be scanned. Attempting OCR...\n",
      "Page 43 might be scanned. Attempting OCR...\n",
      "Page 44 might be scanned. Attempting OCR...\n",
      "Page 45 might be scanned. Attempting OCR...\n",
      "Page 46 might be scanned. Attempting OCR...\n",
      "Page 47 might be scanned. Attempting OCR...\n",
      "Page 48 might be scanned. Attempting OCR...\n",
      "Page 49 might be scanned. Attempting OCR...\n",
      "Page 50 might be scanned. Attempting OCR...\n",
      "Page 51 might be scanned. Attempting OCR...\n",
      "Page 52 might be scanned. Attempting OCR...\n",
      "Page 53 might be scanned. Attempting OCR...\n",
      "Page 54 might be scanned. Attempting OCR...\n",
      "Page 55 might be scanned. Attempting OCR...\n",
      "Page 56 might be scanned. Attempting OCR...\n",
      "Page 57 might be scanned. Attempting OCR...\n",
      "Page 58 might be scanned. Attempting OCR...\n",
      "Page 59 might be scanned. Attempting OCR...\n",
      "Page 60 might be scanned. Attempting OCR...\n",
      "Page 61 might be scanned. Attempting OCR...\n",
      "Page 62 might be scanned. Attempting OCR...\n",
      "Page 63 might be scanned. Attempting OCR...\n",
      "Page 64 might be scanned. Attempting OCR...\n",
      "Page 65 might be scanned. Attempting OCR...\n",
      "Page 66 might be scanned. Attempting OCR...\n",
      "Page 67 might be scanned. Attempting OCR...\n",
      "Page 68 might be scanned. Attempting OCR...\n",
      "Page 69 might be scanned. Attempting OCR...\n",
      "Page 70 might be scanned. Attempting OCR...\n",
      "Page 71 might be scanned. Attempting OCR...\n",
      "Page 72 might be scanned. Attempting OCR...\n",
      "Page 73 might be scanned. Attempting OCR...\n",
      "Page 74 might be scanned. Attempting OCR...\n",
      "Page 75 might be scanned. Attempting OCR...\n",
      "Page 76 might be scanned. Attempting OCR...\n",
      "Page 77 might be scanned. Attempting OCR...\n",
      "Page 78 might be scanned. Attempting OCR...\n",
      "Page 79 might be scanned. Attempting OCR...\n",
      "Page 80 might be scanned. Attempting OCR...\n",
      "Page 81 might be scanned. Attempting OCR...\n",
      "Page 82 might be scanned. Attempting OCR...\n",
      "Page 83 might be scanned. Attempting OCR...\n",
      "Page 84 might be scanned. Attempting OCR...\n",
      "Page 85 might be scanned. Attempting OCR...\n",
      "Page 86 might be scanned. Attempting OCR...\n",
      "Page 87 might be scanned. Attempting OCR...\n",
      "Page 88 might be scanned. Attempting OCR...\n",
      "Page 89 might be scanned. Attempting OCR...\n",
      "Page 90 might be scanned. Attempting OCR...\n",
      "Page 91 might be scanned. Attempting OCR...\n",
      "Page 92 might be scanned. Attempting OCR...\n",
      "Page 93 might be scanned. Attempting OCR...\n",
      "Page 94 might be scanned. Attempting OCR...\n",
      "Page 95 might be scanned. Attempting OCR...\n",
      "Page 96 might be scanned. Attempting OCR...\n",
      "Page 97 might be scanned. Attempting OCR...\n",
      "Page 98 might be scanned. Attempting OCR...\n",
      "Page 99 might be scanned. Attempting OCR...\n",
      "Page 100 might be scanned. Attempting OCR...\n",
      "Page 101 might be scanned. Attempting OCR...\n",
      "Page 102 might be scanned. Attempting OCR...\n",
      "Page 103 might be scanned. Attempting OCR...\n",
      "Page 104 might be scanned. Attempting OCR...\n",
      "Page 105 might be scanned. Attempting OCR...\n",
      "Page 106 might be scanned. Attempting OCR...\n",
      "Page 107 might be scanned. Attempting OCR...\n",
      "Page 108 might be scanned. Attempting OCR...\n",
      "Page 109 might be scanned. Attempting OCR...\n",
      "Page 110 might be scanned. Attempting OCR...\n",
      "Page 111 might be scanned. Attempting OCR...\n",
      "Page 112 might be scanned. Attempting OCR...\n",
      "Page 113 might be scanned. Attempting OCR...\n",
      "Page 114 might be scanned. Attempting OCR...\n",
      "Page 115 might be scanned. Attempting OCR...\n",
      "Page 116 might be scanned. Attempting OCR...\n",
      "Page 117 might be scanned. Attempting OCR...\n",
      "Page 118 might be scanned. Attempting OCR...\n",
      "Page 119 might be scanned. Attempting OCR...\n",
      "Page 120 might be scanned. Attempting OCR...\n",
      "Page 121 might be scanned. Attempting OCR...\n",
      "Page 122 might be scanned. Attempting OCR...\n",
      "Page 123 might be scanned. Attempting OCR...\n",
      "Page 124 might be scanned. Attempting OCR...\n",
      "Page 125 might be scanned. Attempting OCR...\n",
      "Page 126 might be scanned. Attempting OCR...\n",
      "Page 127 might be scanned. Attempting OCR...\n",
      "Page 128 might be scanned. Attempting OCR...\n",
      "Page 129 might be scanned. Attempting OCR...\n",
      "Page 130 might be scanned. Attempting OCR...\n",
      "Page 131 might be scanned. Attempting OCR...\n",
      "Page 132 might be scanned. Attempting OCR...\n",
      "Page 133 might be scanned. Attempting OCR...\n",
      "Page 134 might be scanned. Attempting OCR...\n",
      "Page 135 might be scanned. Attempting OCR...\n",
      "Page 136 might be scanned. Attempting OCR...\n",
      "Page 137 might be scanned. Attempting OCR...\n",
      "Page 138 might be scanned. Attempting OCR...\n",
      "Page 139 might be scanned. Attempting OCR...\n",
      "Page 140 might be scanned. Attempting OCR...\n",
      "Page 141 might be scanned. Attempting OCR...\n",
      "Page 142 might be scanned. Attempting OCR...\n",
      "Page 143 might be scanned. Attempting OCR...\n",
      "Page 144 might be scanned. Attempting OCR...\n",
      "Page 145 might be scanned. Attempting OCR...\n",
      "Page 146 might be scanned. Attempting OCR...\n",
      "Page 147 might be scanned. Attempting OCR...\n",
      "Page 148 might be scanned. Attempting OCR...\n",
      "Page 149 might be scanned. Attempting OCR...\n",
      "Page 150 might be scanned. Attempting OCR...\n",
      "Page 151 might be scanned. Attempting OCR...\n",
      "Page 152 might be scanned. Attempting OCR...\n",
      "Page 153 might be scanned. Attempting OCR...\n",
      "Page 154 might be scanned. Attempting OCR...\n",
      "Page 155 might be scanned. Attempting OCR...\n",
      "Page 156 might be scanned. Attempting OCR...\n",
      "Page 157 might be scanned. Attempting OCR...\n",
      "Page 158 might be scanned. Attempting OCR...\n",
      "Page 159 might be scanned. Attempting OCR...\n",
      "Page 160 might be scanned. Attempting OCR...\n",
      "Page 161 might be scanned. Attempting OCR...\n",
      "Page 162 might be scanned. Attempting OCR...\n",
      "Page 163 might be scanned. Attempting OCR...\n",
      "Page 164 might be scanned. Attempting OCR...\n",
      "Page 165 might be scanned. Attempting OCR...\n",
      "Page 166 might be scanned. Attempting OCR...\n",
      "Page 167 might be scanned. Attempting OCR...\n",
      "Page 168 might be scanned. Attempting OCR...\n",
      "Page 169 might be scanned. Attempting OCR...\n",
      "Page 170 might be scanned. Attempting OCR...\n",
      "Page 171 might be scanned. Attempting OCR...\n",
      "Page 172 might be scanned. Attempting OCR...\n",
      "Page 173 might be scanned. Attempting OCR...\n",
      "Page 174 might be scanned. Attempting OCR...\n",
      "Page 175 might be scanned. Attempting OCR...\n",
      "Page 176 might be scanned. Attempting OCR...\n",
      "Page 177 might be scanned. Attempting OCR...\n",
      "Page 178 might be scanned. Attempting OCR...\n",
      "Page 179 might be scanned. Attempting OCR...\n",
      "Page 180 might be scanned. Attempting OCR...\n",
      "Page 181 might be scanned. Attempting OCR...\n",
      "Page 182 might be scanned. Attempting OCR...\n",
      "Page 183 might be scanned. Attempting OCR...\n",
      "Page 184 might be scanned. Attempting OCR...\n",
      "Page 185 might be scanned. Attempting OCR...\n",
      "Page 186 might be scanned. Attempting OCR...\n",
      "Page 187 might be scanned. Attempting OCR...\n",
      "Page 188 might be scanned. Attempting OCR...\n",
      "Page 189 might be scanned. Attempting OCR...\n",
      "Page 190 might be scanned. Attempting OCR...\n",
      "Page 191 might be scanned. Attempting OCR...\n",
      "Page 192 might be scanned. Attempting OCR...\n",
      "Page 193 might be scanned. Attempting OCR...\n",
      "Page 194 might be scanned. Attempting OCR...\n",
      "Page 195 might be scanned. Attempting OCR...\n",
      "Page 196 might be scanned. Attempting OCR...\n",
      "Page 197 might be scanned. Attempting OCR...\n",
      "Page 198 might be scanned. Attempting OCR...\n",
      "Page 199 might be scanned. Attempting OCR...\n",
      "Page 200 might be scanned. Attempting OCR...\n",
      "Page 201 might be scanned. Attempting OCR...\n",
      "Page 202 might be scanned. Attempting OCR...\n",
      "Page 203 might be scanned. Attempting OCR...\n",
      "Page 204 might be scanned. Attempting OCR...\n",
      "Page 205 might be scanned. Attempting OCR...\n",
      "Page 206 might be scanned. Attempting OCR...\n",
      "Page 207 might be scanned. Attempting OCR...\n",
      "Page 208 might be scanned. Attempting OCR...\n",
      "Page 209 might be scanned. Attempting OCR...\n",
      "Page 210 might be scanned. Attempting OCR...\n",
      "Page 211 might be scanned. Attempting OCR...\n",
      "Page 212 might be scanned. Attempting OCR...\n",
      "Page 213 might be scanned. Attempting OCR...\n",
      "Page 214 might be scanned. Attempting OCR...\n",
      "Page 215 might be scanned. Attempting OCR...\n",
      "Page 216 might be scanned. Attempting OCR...\n",
      "Page 217 might be scanned. Attempting OCR...\n",
      "Page 218 might be scanned. Attempting OCR...\n",
      "Page 219 might be scanned. Attempting OCR...\n",
      "Page 220 might be scanned. Attempting OCR...\n",
      "Page 221 might be scanned. Attempting OCR...\n",
      "Page 222 might be scanned. Attempting OCR...\n",
      "Page 223 might be scanned. Attempting OCR...\n",
      "Page 224 might be scanned. Attempting OCR...\n",
      "Page 225 might be scanned. Attempting OCR...\n",
      "Page 226 might be scanned. Attempting OCR...\n",
      "Page 227 might be scanned. Attempting OCR...\n",
      "Page 228 might be scanned. Attempting OCR...\n",
      "Page 229 might be scanned. Attempting OCR...\n",
      "Page 230 might be scanned. Attempting OCR...\n",
      "Page 231 might be scanned. Attempting OCR...\n",
      "Page 232 might be scanned. Attempting OCR...\n",
      "Page 233 might be scanned. Attempting OCR...\n",
      "Page 234 might be scanned. Attempting OCR...\n",
      "Page 235 might be scanned. Attempting OCR...\n",
      "Page 236 might be scanned. Attempting OCR...\n",
      "Page 237 might be scanned. Attempting OCR...\n",
      "Page 238 might be scanned. Attempting OCR...\n",
      "Page 239 might be scanned. Attempting OCR...\n",
      "Page 240 might be scanned. Attempting OCR...\n",
      "Page 241 might be scanned. Attempting OCR...\n",
      "Page 242 might be scanned. Attempting OCR...\n",
      "Page 243 might be scanned. Attempting OCR...\n",
      "Page 244 might be scanned. Attempting OCR...\n",
      "Page 245 might be scanned. Attempting OCR...\n",
      "Page 246 might be scanned. Attempting OCR...\n",
      "Page 247 might be scanned. Attempting OCR...\n",
      "Page 248 might be scanned. Attempting OCR...\n",
      "Page 249 might be scanned. Attempting OCR...\n",
      "Page 250 might be scanned. Attempting OCR...\n",
      "Page 251 might be scanned. Attempting OCR...\n",
      "Page 252 might be scanned. Attempting OCR...\n",
      "Page 253 might be scanned. Attempting OCR...\n",
      "Page 254 might be scanned. Attempting OCR...\n",
      "Page 255 might be scanned. Attempting OCR...\n",
      "Page 256 might be scanned. Attempting OCR...\n",
      "Page 257 might be scanned. Attempting OCR...\n",
      "Page 258 might be scanned. Attempting OCR...\n",
      "Page 259 might be scanned. Attempting OCR...\n",
      "Page 260 might be scanned. Attempting OCR...\n",
      "Page 261 might be scanned. Attempting OCR...\n",
      "Page 262 might be scanned. Attempting OCR...\n",
      "Page 263 might be scanned. Attempting OCR...\n",
      "Page 264 might be scanned. Attempting OCR...\n",
      "Page 265 might be scanned. Attempting OCR...\n",
      "Page 266 might be scanned. Attempting OCR...\n",
      "Page 267 might be scanned. Attempting OCR...\n",
      "Page 268 might be scanned. Attempting OCR...\n",
      "Page 269 might be scanned. Attempting OCR...\n",
      "Page 270 might be scanned. Attempting OCR...\n",
      "Page 271 might be scanned. Attempting OCR...\n",
      "Page 272 might be scanned. Attempting OCR...\n",
      "Page 273 might be scanned. Attempting OCR...\n",
      "Page 274 might be scanned. Attempting OCR...\n",
      "Page 275 might be scanned. Attempting OCR...\n",
      "Page 276 might be scanned. Attempting OCR...\n",
      "Page 277 might be scanned. Attempting OCR...\n",
      "Page 278 might be scanned. Attempting OCR...\n",
      "Page 279 might be scanned. Attempting OCR...\n",
      "Page 280 might be scanned. Attempting OCR...\n",
      "Page 281 might be scanned. Attempting OCR...\n",
      "Page 282 might be scanned. Attempting OCR...\n",
      "Page 283 might be scanned. Attempting OCR...\n",
      "Page 284 might be scanned. Attempting OCR...\n",
      "Page 285 might be scanned. Attempting OCR...\n",
      "Page 286 might be scanned. Attempting OCR...\n",
      "Page 287 might be scanned. Attempting OCR...\n",
      "Page 288 might be scanned. Attempting OCR...\n",
      "Page 289 might be scanned. Attempting OCR...\n",
      "Page 290 might be scanned. Attempting OCR...\n",
      "Page 291 might be scanned. Attempting OCR...\n",
      "Page 292 might be scanned. Attempting OCR...\n",
      "Page 293 might be scanned. Attempting OCR...\n",
      "Page 294 might be scanned. Attempting OCR...\n",
      "Page 295 might be scanned. Attempting OCR...\n",
      "Page 296 might be scanned. Attempting OCR...\n",
      "Page 297 might be scanned. Attempting OCR...\n",
      "Page 298 might be scanned. Attempting OCR...\n",
      "Page 299 might be scanned. Attempting OCR...\n",
      "Page 300 might be scanned. Attempting OCR...\n",
      "Page 301 might be scanned. Attempting OCR...\n",
      "Page 302 might be scanned. Attempting OCR...\n",
      "Page 303 might be scanned. Attempting OCR...\n",
      "Page 304 might be scanned. Attempting OCR...\n",
      "Page 305 might be scanned. Attempting OCR...\n",
      "Page 306 might be scanned. Attempting OCR...\n",
      "Page 307 might be scanned. Attempting OCR...\n",
      "Page 308 might be scanned. Attempting OCR...\n",
      "Page 309 might be scanned. Attempting OCR...\n",
      "Page 310 might be scanned. Attempting OCR...\n",
      "Page 311 might be scanned. Attempting OCR...\n",
      "Page 312 might be scanned. Attempting OCR...\n",
      "Page 313 might be scanned. Attempting OCR...\n",
      "Page 314 might be scanned. Attempting OCR...\n",
      "Page 315 might be scanned. Attempting OCR...\n",
      "Page 316 might be scanned. Attempting OCR...\n",
      "Page 317 might be scanned. Attempting OCR...\n",
      "Page 318 might be scanned. Attempting OCR...\n",
      "Page 319 might be scanned. Attempting OCR...\n",
      "Page 320 might be scanned. Attempting OCR...\n",
      "Page 321 might be scanned. Attempting OCR...\n",
      "Page 322 might be scanned. Attempting OCR...\n",
      "Page 323 might be scanned. Attempting OCR...\n",
      "Page 324 might be scanned. Attempting OCR...\n",
      "Page 325 might be scanned. Attempting OCR...\n",
      "Page 326 might be scanned. Attempting OCR...\n",
      "Page 327 might be scanned. Attempting OCR...\n",
      "Page 328 might be scanned. Attempting OCR...\n",
      "Page 329 might be scanned. Attempting OCR...\n",
      "Page 330 might be scanned. Attempting OCR...\n",
      "Page 331 might be scanned. Attempting OCR...\n",
      "Page 332 might be scanned. Attempting OCR...\n",
      "Page 333 might be scanned. Attempting OCR...\n",
      "Page 334 might be scanned. Attempting OCR...\n",
      "Page 335 might be scanned. Attempting OCR...\n",
      "Page 336 might be scanned. Attempting OCR...\n",
      "Page 337 might be scanned. Attempting OCR...\n",
      "Page 338 might be scanned. Attempting OCR...\n",
      "Page 339 might be scanned. Attempting OCR...\n",
      "Page 340 might be scanned. Attempting OCR...\n",
      "Page 341 might be scanned. Attempting OCR...\n",
      "Cleaning extracted text...\n",
      "Text saved to Apoha-Buddhist Nominalism and Human Cognition.txt\n",
      "\n",
      "Sample of extracted text:\n",
      "° ° ° ° ° °\n",
      "PRBOAORUAUBUBUBUBURBUAUAUA UALR ACIS\n",
      "APOHA\n",
      "BUDDHIST NOMINALISM\n",
      "AND HUMAN COGNITION\n",
      "EDITED BY\n",
      "Mark Siderits\n",
      "Tom Tillemans\n",
      "Arindam Chakrabarti\n",
      "Columbia University Press New York\n",
      "wy\n",
      "Columbia University Press\n",
      "Publishers Since 1893\n",
      "New York Chichester, West Sussex\n",
      "Copyright © 2011 Columbia University Press\n",
      "All rights reserved\n",
      "Library of Congress Cataloging-in-Publication Data\n",
      "Apoha : Buddhist nominalism and human cognition / edited by Mark Siderits, Tom\n",
      "Tillemans, and Arindam Chakrabarti....\n",
      "\n",
      "Total characters: 896995\n",
      "Total tokens: 222143\n",
      "Split into 28 chunks\n",
      "Creating embeddings for 28 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232c5f4c0acf4a0fb985532757e054dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to Apoha-Buddhist Nominalism and Human Cognition_embeddings.json\n",
      "Text chunks saved to Apoha-Buddhist Nominalism and Human Cognition_chunks.json\n",
      "\n",
      "Processing: Dan Arnold - Brains, Buddhas, and Believing - The Problem of Intentionality in Classical Buddhist and Cognitive-Scientific Philo.pdf\n",
      "Extracting text from Dan Arnold - Brains, Buddhas, and Believing - The Problem of Intentionality in Classical Buddhist and Cognitive-Scientific Philo.pdf...\n",
      "Extracting text from 316 pages...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebe616c0daa4abc846f62750ddcdd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/316 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 might be scanned. Attempting OCR...\n",
      "Page 316 might be scanned. Attempting OCR...\n",
      "Cleaning extracted text...\n",
      "Text saved to Dan Arnold - Brains, Buddhas, and Believing - The Problem of Intentionality in Classical Buddhist and Cognitive-Scientific Philo.txt\n",
      "\n",
      "Sample of extracted text:\n",
      "THE PROBLEM OF INTENTIONALITY IN CLASSICAL BUDDHIST AND\n",
      "COGNITIVE-SCIENTIFIC PHILOSOPHY OF MIND\n",
      "Dan Arnold \n",
      "Brains, Buddhas, and Believing \n",
      "THE PROBLEM OF INTENTIONALITY \n",
      "IN CLASSICAL BUDDHIST AND COGNITIVE­\n",
      "SCIENTIFIC PHILOSOPHY OF MIND \n",
      "columbia University Press \n",
      "New¥ork \n",
      "COLUMBIA UNIVERSITY PRESS \n",
      "Publishers Since 1893 \n",
      "New York Chichester, West Sussex \n",
      "cup.columbia.edu \n",
      "Copyright © 2012 Columbia University Press \n",
      "All rights reserved \n",
      "Library of Congress cataloging-in-Publication Data \n",
      "Arnold...\n",
      "\n",
      "Total characters: 873513\n",
      "Total tokens: 224528\n",
      "Split into 29 chunks\n",
      "Creating embeddings for 29 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3bc7bfe7da4ed289245285c1216ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to Dan Arnold - Brains, Buddhas, and Believing - The Problem of Intentionality in Classical Buddhist and Cognitive-Scientific Philo_embeddings.json\n",
      "Text chunks saved to Dan Arnold - Brains, Buddhas, and Believing - The Problem of Intentionality in Classical Buddhist and Cognitive-Scientific Philo_chunks.json\n",
      "\n",
      "Processing: Dreyfus_RecognizingReality.pdf\n",
      "Extracting text from Dreyfus_RecognizingReality.pdf...\n",
      "Extracting text from 643 pages...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff7111d72a44b9e9cb7c3bf003844ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/643 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 9 might be scanned. Attempting OCR...\n",
      "Page 13 might be scanned. Attempting OCR...\n",
      "Page 21 might be scanned. Attempting OCR...\n",
      "Page 35 might be scanned. Attempting OCR...\n",
      "Page 63 might be scanned. Attempting OCR...\n",
      "Page 65 might be scanned. Attempting OCR...\n",
      "Page 67 might be scanned. Attempting OCR...\n",
      "Page 147 might be scanned. Attempting OCR...\n",
      "Page 225 might be scanned. Attempting OCR...\n",
      "Page 303 might be scanned. Attempting OCR...\n",
      "Page 305 might be scanned. Attempting OCR...\n",
      "Page 349 might be scanned. Attempting OCR...\n",
      "Page 351 might be scanned. Attempting OCR...\n",
      "Page 583 might be scanned. Attempting OCR...\n",
      "Page 595 might be scanned. Attempting OCR...\n",
      "Page 623 might be scanned. Attempting OCR...\n",
      "Page 631 might be scanned. Attempting OCR...\n",
      "Cleaning extracted text...\n",
      "Text saved to Dreyfus_RecognizingReality.txt\n",
      "\n",
      "Sample of extracted text:\n",
      "SUNY Series in Buddhist Studies \n",
      "Matthew Kapstein, editor \n",
      "· RECtJCiNIZING REALITY \n",
      "Dharmakirti s Philosophy \n",
      "and Its Tibetan Interpretations \n",
      "Georges B. J. Dreyfus \n",
      "State University of New York Press \n",
      "APR 041997 \n",
      "Published by \n",
      "State University of New York Press, Albany \n",
      "© 1997 State University of New York \n",
      "All rights reserved \n",
      "Printed in the United States of America \n",
      ",, \n",
      "No part of this book may be used or reproduced in any manner \n",
      "whatsoever without written permission. No part of this book may...\n",
      "\n",
      "Total characters: 1972491\n",
      "Total tokens: 498089\n",
      "Split into 63 chunks\n",
      "Creating embeddings for 63 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56db7cea202346d19bae5ab2334675c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to Dreyfus_RecognizingReality_embeddings.json\n",
      "Text chunks saved to Dreyfus_RecognizingReality_chunks.json\n",
      "\n",
      "Processing: Dunne_Foundations FDP smaller file.pdf\n",
      "Extracting text from Dunne_Foundations FDP smaller file.pdf...\n",
      "Extracting text from 245 pages...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2544fdafd9044e588c2b280e1e5a01a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning extracted text...\n",
      "Text saved to Dunne_Foundations FDP smaller file.txt\n",
      "\n",
      "Sample of extracted text:\n",
      "FOUNDATIONS OF DHARMAKÊRTI’S PHILOSOPHY\n",
      "Studies in Indian and Tibetan Buddhism\n",
      "This series was conceived to provide a forum for publishing outstanding new contributions to scholarship on\n",
      "Indian and Tibetan Buddhism and also to make accessible seminal research not widely known outside a narrow\n",
      "specialist audience, including translations of appropriate\n",
      "monographs and collections of articles from other languages. The series strives to shed light on the Indic Buddhist traditions by exposing them to ...\n",
      "\n",
      "Total characters: 1279735\n",
      "Total tokens: 347335\n",
      "Split into 44 chunks\n",
      "Creating embeddings for 44 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bc52b9e3ef41a19698bdd81ce9d830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to Dunne_Foundations FDP smaller file_embeddings.json\n",
      "Text chunks saved to Dunne_Foundations FDP smaller file_chunks.json\n",
      "\n",
      "Processing: Dunne_J_Key_Features_of_Dharmakirtis_Apoha_Theory.pdf\n",
      "Extracting text from Dunne_J_Key_Features_of_Dharmakirtis_Apoha_Theory.pdf...\n",
      "Extracting text from 25 pages...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17449655e934b469f7dfae068d106cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning extracted text...\n",
      "Text saved to Dunne_J_Key_Features_of_Dharmakirtis_Apoha_Theory.txt\n",
      "\n",
      "Sample of extracted text:\n",
      "T\n",
      "he apoha theory contains a number of occasionally technical and \n",
      "even counterintuitive elements, and the main purpose of this chapter is to present its most fundamental features in a straightforward \n",
      "fashion. At the outset it is critical to note that, while certainly uniﬁ ed in its \n",
      "overall scope, the apoha theory underwent historical development that led \n",
      "to divergent interpretations among its formulators, and any single, uniﬁ ed \n",
      "account of the theory would be problematic. Hence, this chapte...\n",
      "\n",
      "Total characters: 71538\n",
      "Total tokens: 19298\n",
      "Split into 3 chunks\n",
      "Creating embeddings for 3 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8a577ff3aa45ccaf35798bf0efa34c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to Dunne_J_Key_Features_of_Dharmakirtis_Apoha_Theory_embeddings.json\n",
      "Text chunks saved to Dunne_J_Key_Features_of_Dharmakirtis_Apoha_Theory_chunks.json\n",
      "\n",
      "Processing: Jake H. Davis, Owen Flanagan - A mirror is for reflection _ understanding Buddhist ethics-Oxford University Press (2017).pdf\n",
      "Extracting text from Jake H. Davis, Owen Flanagan - A mirror is for reflection _ understanding Buddhist ethics-Oxford University Press (2017).pdf...\n",
      "Extracting text from 393 pages...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93b55a1c36543ffa653b82d3d634574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 might be scanned. Attempting OCR...\n",
      "Page 3 might be scanned. Attempting OCR...\n",
      "Page 7 might be scanned. Attempting OCR...\n",
      "Page 11 might be scanned. Attempting OCR...\n",
      "Page 19 might be scanned. Attempting OCR...\n",
      "Page 21 might be scanned. Attempting OCR...\n",
      "Page 27 might be scanned. Attempting OCR...\n",
      "Page 29 might be scanned. Attempting OCR...\n",
      "Page 43 might be scanned. Attempting OCR...\n",
      "Page 45 might be scanned. Attempting OCR...\n",
      "Page 99 might be scanned. Attempting OCR...\n",
      "Page 101 might be scanned. Attempting OCR...\n",
      "Page 159 might be scanned. Attempting OCR...\n",
      "Page 213 might be scanned. Attempting OCR...\n",
      "Page 265 might be scanned. Attempting OCR...\n",
      "Page 267 might be scanned. Attempting OCR...\n",
      "Page 325 might be scanned. Attempting OCR...\n",
      "Page 381 might be scanned. Attempting OCR...\n",
      "Page 388 might be scanned. Attempting OCR...\n",
      "Page 389 might be scanned. Attempting OCR...\n",
      "Page 390 might be scanned. Attempting OCR...\n",
      "Page 391 might be scanned. Attempting OCR...\n",
      "Page 392 might be scanned. Attempting OCR...\n",
      "Page 393 might be scanned. Attempting OCR...\n",
      "Cleaning extracted text...\n",
      "Text saved to Jake H. Davis, Owen Flanagan - A mirror is for reflection _ understanding Buddhist ethics-Oxford University Press (2017).txt\n",
      "\n",
      "Sample of extracted text:\n",
      "Edited by\n",
      "JAKE H. DAVIS\n",
      "A Mirror\n",
      "IS FOR\n",
      "Reflection\n",
      "UNDERSTANDING\n",
      "BUDDHIST\n",
      "ETHICS\n",
      "i\n",
      "A Mirror Is for Reflection\n",
      " \n",
      "iii\n",
      "1\n",
      "A Mirror Is \n",
      "for Reflection\n",
      "Understanding Buddhist Ethics\n",
      "Edited by\n",
      "JAKE H. DAVIS\n",
      " \n",
      "3\n",
      "iv\n",
      "Oxford University Press is a department of the University of Oxford. It furthers\n",
      "the University’s objective of excellence in research, scholarship, and education\n",
      "by publishing worldwide. Oxford is a registered trade mark of Oxford University\n",
      "Press in the UK and certain other countries.\n",
      "Publis...\n",
      "\n",
      "Total characters: 949803\n",
      "Total tokens: 227034\n",
      "Split into 29 chunks\n",
      "Creating embeddings for 29 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3901f30d91aa4b36a0168152499cb444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to Jake H. Davis, Owen Flanagan - A mirror is for reflection _ understanding Buddhist ethics-Oxford University Press (2017)_embeddings.json\n",
      "Text chunks saved to Jake H. Davis, Owen Flanagan - A mirror is for reflection _ understanding Buddhist ethics-Oxford University Press (2017)_chunks.json\n",
      "\n",
      "Processing: Peter-Woods-MA-Thesis-2022.pdf\n",
      "Extracting text from Peter-Woods-MA-Thesis-2022.pdf...\n",
      "Extracting text from 121 pages...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2394244ad92040648c990d3192e3d628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning extracted text...\n",
      "Text saved to Peter-Woods-MA-Thesis-2022.txt\n",
      "\n",
      "Sample of extracted text:\n",
      "Echoes of Awakening: \n",
      "Reimagining Liberation in the New Treasures of Chokgyur Lingpa \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Peter F. Woods \n",
      "Kathmandu University, Centre for Buddhist Studies at Rangjung Yeshe Institute \n",
      "Master of Arts (MA) in Buddhist Studies \n",
      "September 30, 2021 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "2 \n",
      "Table of Contents \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1. Introducing the Lotus Essence Tantra \n",
      "1.1 Introduction \n",
      "1.2 Outline \n",
      "1.3 Literature Review \n",
      "1.4 Methodology \n",
      "2. A Brief History of Liberating Scriptures (and Other Materials) in Indian and Tibetan \n",
      "Buddh...\n",
      "\n",
      "Total characters: 291176\n",
      "Total tokens: 78329\n",
      "Split into 10 chunks\n",
      "Creating embeddings for 10 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08eabfa4a3ba4ab4bba48faef925cd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to Peter-Woods-MA-Thesis-2022_embeddings.json\n",
      "Text chunks saved to Peter-Woods-MA-Thesis-2022_chunks.json\n",
      "\n",
      "===== Processing Summary =====\n",
      "Total files: 7\n",
      "Successfully processed: 7\n",
      "Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# Process files based on settings\n",
    "if batch_process:\n",
    "    # Find all PDF files matching the pattern\n",
    "    pdf_files = glob.glob(pdf_pattern)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found matching pattern: {pdf_pattern}\")\n",
    "    else:\n",
    "        print(f\"Found {len(pdf_files)} PDF files matching pattern: {pdf_pattern}\")\n",
    "        \n",
    "        # Process each PDF file\n",
    "        results = []\n",
    "        for pdf_file in pdf_files:\n",
    "            success = process_pdf(pdf_file)\n",
    "            results.append((pdf_file, success))\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n===== Processing Summary =====\")\n",
    "        print(f\"Total files: {len(results)}\")\n",
    "        successful = sum(1 for _, success in results if success)\n",
    "        print(f\"Successfully processed: {successful}\")\n",
    "        print(f\"Failed: {len(results) - successful}\")\n",
    "        \n",
    "        if len(results) - successful > 0:\n",
    "            print(\"\\nFailed files:\")\n",
    "            for pdf_file, success in results:\n",
    "                if not success:\n",
    "                    print(f\"- {pdf_file}\")\n",
    "else:\n",
    "    # Process a single file\n",
    "    process_pdf(single_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Example: Using the Semantic Search Feature\n",
    "\n",
    "\n",
    "\n",
    " After generating embeddings, you can use this code to search within your PDF content.\n",
    "\n",
    " Uncomment and modify this code when you're ready to search your embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for semantic search - uncomment and modify when needed\n",
    "# Load embeddings from file\n",
    "# embedding_file = \"your_document_embeddings.json\"  # Replace with your actual embeddings file\n",
    "# with open(embedding_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#     embeddings = json.load(f)\n",
    "\n",
    "# Search for relevant content - uses API key from .env by default\n",
    "# query = \"Enter your search query here\"  # Replace with your actual search query\n",
    "# results = search_embeddings(query, embeddings, top_n=3)\n",
    "\n",
    "# Display results\n",
    "# print(f\"Search results for: {query}\\n\")\n",
    "# for i, result in enumerate(results):\n",
    "#     print(f\"Result {i+1} (Similarity: {result['similarity']:.4f}):\")\n",
    "#     print(\"-\" * 40)\n",
    "#     print(result[\"chunk\"][:300] + \"...\")  # Show first 300 chars\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Examples: Batch Processing Patterns\n",
    "\n",
    "\n",
    "\n",
    " Here are examples of different glob patterns you can use for batch processing.\n",
    "\n",
    " Uncomment and modify these examples when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example glob patterns for different scenarios - uncomment and modify when needed\n",
    "\n",
    "# Process all PDFs in the current directory\n",
    "pdf_pattern = \"*.pdf\"\n",
    "\n",
    "# Process PDFs with specific naming pattern\n",
    "# pdf_pattern = \"report_*.pdf\"\n",
    "\n",
    "# Process PDFs in a specific directory\n",
    "# pdf_pattern = \"documents/*.pdf\"\n",
    "\n",
    "# Process PDFs in a specific directory and subdirectories (recursive)\n",
    "# pdf_pattern = \"documents/**/*.pdf\"  # Note: requires Python 3.5+\n",
    "\n",
    "# Process PDFs from multiple directories\n",
    "# pdf_files = []\n",
    "# pdf_files.extend(glob.glob(\"reports/*.pdf\"))\n",
    "# pdf_files.extend(glob.glob(\"archives/*.pdf\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Creating a .env File\n",
    "\n",
    "\n",
    "\n",
    " Create a file named `.env` in the same directory as this notebook with the following content:\n",
    "\n",
    "\n",
    "\n",
    " ```\n",
    "\n",
    " OPENAI_API_KEY=your_api_key_here\n",
    "\n",
    " ```\n",
    "\n",
    "\n",
    "\n",
    " This file will be automatically loaded when you run the notebook, and the API key will be available\n",
    "\n",
    " without having to hardcode it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
