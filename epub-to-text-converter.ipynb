{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # EPUB to Text Converter with OpenAI Embeddings\n",
    "\n",
    "\n",
    "\n",
    " This notebook extracts text from EPUB files and can generate embeddings using OpenAI's API.\n",
    "\n",
    " The extracted text is saved to a text file, and optionally embeddings can be generated\n",
    "\n",
    " for the content to enable semantic search or analysis.\n",
    "\n",
    "\n",
    "\n",
    " ## Requirements\n",
    "\n",
    " You'll need to install these packages first:\n",
    "\n",
    " ```\n",
    "\n",
    " pip install ebooklib openai tqdm tiktoken numpy beautifulsoup4 python-dotenv\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "from tqdm.notebook import tqdm  # Using notebook version for better display in Jupyter\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file in the current directory\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment variable\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"  # OpenAI embedding model to use\n",
    "MAX_TOKENS = 8191  # Maximum tokens per chunk for embedding\n",
    "CHUNK_OVERLAP = 200  # Number of tokens to overlap between chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epub_to_text(epub_path):\n",
    "    \"\"\"Extract text content from an EPUB file\"\"\"\n",
    "    try:\n",
    "        book = epub.read_epub(epub_path)\n",
    "        chapters = []\n",
    "        \n",
    "        for item in book.get_items():\n",
    "            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "                # Get content from each document item\n",
    "                content = item.get_content().decode('utf-8')\n",
    "                # Use BeautifulSoup to extract text from HTML\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "                text = soup.get_text()\n",
    "                # Clean up the text\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                if text:\n",
    "                    chapters.append(text)\n",
    "        \n",
    "        return \"\\n\\n\".join(chapters)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing EPUB file: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_count(text, encoding_name=\"cl100k_base\"):\n",
    "    \"\"\"Count the number of tokens in a text string\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text, max_tokens=MAX_TOKENS, overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"Split text into chunks respecting token limits with overlap\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    \n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Get chunk of tokens (respecting max_tokens)\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        # Decode chunk back to text\n",
    "        chunk = encoding.decode(chunk_tokens)\n",
    "        chunks.append(chunk)\n",
    "        # Move forward by max_tokens - overlap\n",
    "        i += max_tokens - overlap\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(chunks, api_key=None):\n",
    "    \"\"\"Create embeddings for text chunks using OpenAI API\"\"\"\n",
    "    # Use provided API key or environment variable\n",
    "    if api_key:\n",
    "        openai.api_key = api_key\n",
    "    else:\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "        \n",
    "    if not openai.api_key:\n",
    "        print(\"Error: No OpenAI API key provided. Set OPENAI_API_KEY in your .env file.\")\n",
    "        return []\n",
    "        \n",
    "    embeddings = []\n",
    "    \n",
    "    print(f\"Creating embeddings for {len(chunks)} chunks...\")\n",
    "    for i, chunk in enumerate(tqdm(chunks)):\n",
    "        try:\n",
    "            # Add a small delay to respect API rate limits\n",
    "            if i > 0 and i % 10 == 0:\n",
    "                time.sleep(1)\n",
    "                \n",
    "            response = openai.embeddings.create(\n",
    "                model=EMBEDDING_MODEL,\n",
    "                input=chunk\n",
    "            )\n",
    "            embedding = response.data[0].embedding\n",
    "            embeddings.append({\n",
    "                \"chunk\": chunk,\n",
    "                \"embedding\": embedding,\n",
    "                \"chunk_index\": i\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating embedding for chunk {i}: {e}\")\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Interactive Notebook Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive part - you can edit these parameters\n",
    "epub_file = \"myepub.epub\"  # Path to your EPUB file\n",
    "output_dir = \".\"  # Directory to save output files\n",
    "generate_embeddings = False  # Set to True if you want to generate embeddings\n",
    "\n",
    "# Use API key from environment variable by default\n",
    "# You can override it here if needed\n",
    "openai_api_key = OPENAI_API_KEY\n",
    "\n",
    "# Check if API key is available when embeddings are requested\n",
    "if generate_embeddings and not openai_api_key:\n",
    "    print(\"Warning: No OpenAI API key found in environment variables.\")\n",
    "    print(\"Please add OPENAI_API_KEY to your .env file or set it here manually.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Process the EPUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from myepub.epub...\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = Path(output_dir)\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Get filename without extension\n",
    "filename = Path(epub_file).stem\n",
    "\n",
    "# Convert EPUB to text\n",
    "print(f\"Extracting text from {epub_file}...\")\n",
    "text = epub_to_text(epub_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved to myepub.txt\n",
      "\n",
      "Sample of extracted text:\n",
      "title : A Magic Still Dwells : Comparative Religion in the Postmodern Age author : Patton, Kimberley C. publisher : University of California Press isbn10 | asin : 0520221052 print isbn13 : 9780520221055 ebook isbn13 : 9780585369884 language : English subject Religions, Postmodernism. publication date : 2000 lcc : BL80.2.M278 2000eb ddc : 200/.71 subject : Religions, Postmodernism.\n",
      "\n",
      "Page iii A Magic Still Dwells Comparative Religion in the Postmodern Age Edited by Kimberley C. Patton and Benjamin...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if text:\n",
    "    # Save text content\n",
    "    text_path = output_dir / f\"{filename}.txt\"\n",
    "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"Text saved to {text_path}\")\n",
    "    \n",
    "    # Display a sample of the text\n",
    "    print(\"\\nSample of extracted text:\")\n",
    "    print(text[:500] + \"...\\n\")\n",
    "    \n",
    "    # Create embeddings if requested\n",
    "    if generate_embeddings:\n",
    "        if not openai_api_key:\n",
    "            print(\"Error: OpenAI API key is required for creating embeddings\")\n",
    "            print(\"Please set OPENAI_API_KEY in your .env file or provide it in the parameters cell.\")\n",
    "        else:\n",
    "            # Set API key\n",
    "            openai.api_key = openai_api_key\n",
    "            \n",
    "            # Split text into chunks\n",
    "            token_count = get_token_count(text)\n",
    "            print(f\"Total tokens: {token_count}\")\n",
    "            \n",
    "            chunks = split_into_chunks(text)\n",
    "            print(f\"Split into {len(chunks)} chunks\")\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = create_embeddings(chunks, openai_api_key)\n",
    "            \n",
    "            # Save embeddings\n",
    "            embeddings_path = output_dir / f\"{filename}_embeddings.json\"\n",
    "            with open(embeddings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(embeddings, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Embeddings saved to {embeddings_path}\")\n",
    "            \n",
    "            # Save a version with just the text chunks for reference\n",
    "            chunks_path = output_dir / f\"{filename}_chunks.json\"\n",
    "            chunks_data = [{\"chunk_index\": i, \"chunk\": chunk} for i, chunk in enumerate(chunks)]\n",
    "            with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Text chunks saved to {chunks_path}\")\n",
    "else:\n",
    "    print(\"Failed to extract text from the EPUB file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Add Semantic Search Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_embeddings(query, embeddings, api_key=None, top_n=5):\n",
    "    \"\"\"Search embeddings for relevant text chunks based on a query\"\"\"\n",
    "    # Get embedding for the query\n",
    "    if api_key:\n",
    "        openai.api_key = api_key\n",
    "    else:\n",
    "        # Use environment variable if no API key is provided\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "    \n",
    "    if not openai.api_key:\n",
    "        print(\"Error: No OpenAI API key provided. Set OPENAI_API_KEY in your .env file.\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        response = openai.embeddings.create(\n",
    "            model=EMBEDDING_MODEL,\n",
    "            input=query\n",
    "        )\n",
    "        query_embedding = response.data[0].embedding\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_embedding_array = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        similarities = []\n",
    "        for i, item in enumerate(embeddings):\n",
    "            embed_array = np.array(item[\"embedding\"])\n",
    "            # Cosine similarity\n",
    "            similarity = np.dot(query_embedding_array, embed_array) / (\n",
    "                np.linalg.norm(query_embedding_array) * np.linalg.norm(embed_array)\n",
    "            )\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (highest first)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top N results\n",
    "        results = []\n",
    "        for idx, score in similarities[:top_n]:\n",
    "            results.append({\n",
    "                \"chunk\": embeddings[idx][\"chunk\"],\n",
    "                \"similarity\": float(score),\n",
    "                \"chunk_index\": embeddings[idx][\"chunk_index\"]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Example Usage of Search Functionality\n",
    "\n",
    " Uncomment and modify this code when you're ready to search your embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load embeddings from file\\nembedding_file = \"your_book_embeddings.json\"\\nwith open(embedding_file, \"r\", encoding=\"utf-8\") as f:\\n    embeddings = json.load(f)\\n\\n# Search for relevant content - uses API key from .env by default\\nquery = \"Enter your search query here\"\\nresults = search_embeddings(query, embeddings, top_n=3)\\n\\n# Display results\\nprint(f\"Search results for: {query}\\n\")\\nfor i, result in enumerate(results):\\n    print(f\"Result {i+1} (Similarity: {result[\\'similarity\\']:.4f}):\")\\n    print(\"-\" * 40)\\n    print(result[\"chunk\"][:300] + \"...\")  # Show first 300 chars\\n    print()\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example code for semantic search \n",
    "# Uncomment when you have embeddings to search\n",
    "'''\n",
    "# Load embeddings from file\n",
    "embedding_file = \"your_book_embeddings.json\"\n",
    "with open(embedding_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    embeddings = json.load(f)\n",
    "\n",
    "# Search for relevant content - uses API key from .env by default\n",
    "query = \"Enter your search query here\"\n",
    "results = search_embeddings(query, embeddings, top_n=3)\n",
    "\n",
    "# Display results\n",
    "print(f\"Search results for: {query}\\n\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i+1} (Similarity: {result['similarity']:.4f}):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(result[\"chunk\"][:300] + \"...\")  # Show first 300 chars\n",
    "    print()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Creating a .env File\n",
    "\n",
    "\n",
    "\n",
    " Create a file named `.env` in the same directory as this notebook with the following content:\n",
    "\n",
    "\n",
    "\n",
    " ```\n",
    "\n",
    " OPENAI_API_KEY=your_api_key_here\n",
    "\n",
    " ```\n",
    "\n",
    "\n",
    "\n",
    " This file will be automatically loaded when you run the notebook, and the API key will be available\n",
    "\n",
    " without having to hardcode it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Optional: Command-line Version\n",
    "\n",
    "\n",
    "\n",
    " This cell contains a version of the code that can be run as a command-line script.\n",
    "\n",
    " It's included here for reference, but is commented out since we're using the notebook interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport argparse\\nfrom dotenv import load_dotenv\\n\\n# Load environment variables from .env file\\nload_dotenv()\\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\"Convert EPUB to text and optionally create embeddings\")\\n    parser.add_argument(\"epub_file\", help=\"Path to the EPUB file\")\\n    parser.add_argument(\"--embeddings\", action=\"store_true\", help=\"Generate OpenAI embeddings\")\\n    parser.add_argument(\"--api-key\", help=\"OpenAI API key (if not using .env file)\")\\n    parser.add_argument(\"--output-dir\", default=\".\", help=\"Directory to save output files\")\\n    args = parser.parse_args()\\n    \\n    # Get API key from args or environment variable\\n    api_key = args.api_key or os.getenv(\"OPENAI_API_KEY\", \"\")\\n    \\n    # Create output directory if it doesn\\'t exist\\n    output_dir = Path(args.output_dir)\\n    output_dir.mkdir(exist_ok=True, parents=True)\\n    \\n    # Get filename without extension\\n    filename = Path(args.epub_file).stem\\n    \\n    # Convert EPUB to text\\n    print(f\"Extracting text from {args.epub_file}...\")\\n    text = epub_to_text(args.epub_file)\\n    \\n    if text:\\n        # Save text content\\n        text_path = output_dir / f\"{filename}.txt\"\\n        with open(text_path, \"w\", encoding=\"utf-8\") as f:\\n            f.write(text)\\n        print(f\"Text saved to {text_path}\")\\n        \\n        # Create embeddings if requested\\n        if args.embeddings:\\n            if not api_key:\\n                print(\"Error: OpenAI API key is required for creating embeddings\")\\n                print(\"Set it with --api-key or add OPENAI_API_KEY to your .env file\")\\n                return\\n            \\n            # Split text into chunks\\n            token_count = get_token_count(text)\\n            print(f\"Total tokens: {token_count}\")\\n            \\n            chunks = split_into_chunks(text)\\n            print(f\"Split into {len(chunks)} chunks\")\\n            \\n            # Generate embeddings\\n            embeddings = create_embeddings(chunks, api_key)\\n            \\n            # Save embeddings\\n            embeddings_path = output_dir / f\"{filename}_embeddings.json\"\\n            with open(embeddings_path, \"w\", encoding=\"utf-8\") as f:\\n                json.dump(embeddings, f, ensure_ascii=False, indent=2)\\n            print(f\"Embeddings saved to {embeddings_path}\")\\n            \\n            # Save a version with just the text chunks for reference\\n            chunks_path = output_dir / f\"{filename}_chunks.json\"\\n            chunks_data = [{\"chunk_index\": i, \"chunk\": chunk} for i, chunk in enumerate(chunks)]\\n            with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\\n                json.dump(chunks_data, f, ensure_ascii=False, indent=2)\\n            print(f\"Text chunks saved to {chunks_path}\")\\n    else:\\n        print(\"Failed to extract text from the EPUB file.\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Command-line script version - keep this commented out in the notebook\n",
    "'''\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Convert EPUB to text and optionally create embeddings\")\n",
    "    parser.add_argument(\"epub_file\", help=\"Path to the EPUB file\")\n",
    "    parser.add_argument(\"--embeddings\", action=\"store_true\", help=\"Generate OpenAI embeddings\")\n",
    "    parser.add_argument(\"--api-key\", help=\"OpenAI API key (if not using .env file)\")\n",
    "    parser.add_argument(\"--output-dir\", default=\".\", help=\"Directory to save output files\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Get API key from args or environment variable\n",
    "    api_key = args.api_key or os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Get filename without extension\n",
    "    filename = Path(args.epub_file).stem\n",
    "    \n",
    "    # Convert EPUB to text\n",
    "    print(f\"Extracting text from {args.epub_file}...\")\n",
    "    text = epub_to_text(args.epub_file)\n",
    "    \n",
    "    if text:\n",
    "        # Save text content\n",
    "        text_path = output_dir / f\"{filename}.txt\"\n",
    "        with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"Text saved to {text_path}\")\n",
    "        \n",
    "        # Create embeddings if requested\n",
    "        if args.embeddings:\n",
    "            if not api_key:\n",
    "                print(\"Error: OpenAI API key is required for creating embeddings\")\n",
    "                print(\"Set it with --api-key or add OPENAI_API_KEY to your .env file\")\n",
    "                return\n",
    "            \n",
    "            # Split text into chunks\n",
    "            token_count = get_token_count(text)\n",
    "            print(f\"Total tokens: {token_count}\")\n",
    "            \n",
    "            chunks = split_into_chunks(text)\n",
    "            print(f\"Split into {len(chunks)} chunks\")\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = create_embeddings(chunks, api_key)\n",
    "            \n",
    "            # Save embeddings\n",
    "            embeddings_path = output_dir / f\"{filename}_embeddings.json\"\n",
    "            with open(embeddings_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(embeddings, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Embeddings saved to {embeddings_path}\")\n",
    "            \n",
    "            # Save a version with just the text chunks for reference\n",
    "            chunks_path = output_dir / f\"{filename}_chunks.json\"\n",
    "            chunks_data = [{\"chunk_index\": i, \"chunk\": chunk} for i, chunk in enumerate(chunks)]\n",
    "            with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Text chunks saved to {chunks_path}\")\n",
    "    else:\n",
    "        print(\"Failed to extract text from the EPUB file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
